"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[88341],{28453(e,n,t){t.d(n,{R:()=>r,x:()=>i});var s=t(96540);const o={},a=s.createContext(o);function r(e){const n=s.useContext(a);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),s.createElement(a.Provider,{value:n},e.children)}},62744(e,n,t){t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>u,frontMatter:()=>i,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"workspace/developers/ai-features/mcp-tools","title":"MCP tools integration","description":"Integrate Model Context Protocol (MCP) tools with OpenBB agents for enhanced capabilities","source":"@site/content/workspace/developers/ai-features/mcp-tools.md","sourceDirName":"workspace/developers/ai-features","slug":"/workspace/developers/ai-features/mcp-tools","permalink":"/workspace/developers/ai-features/mcp-tools","draft":false,"unlisted":false,"editUrl":"https://github.com/OpenBB-finance/openbb-docs/edit/main/content/workspace/developers/ai-features/mcp-tools.md","tags":[],"version":"current","lastUpdatedBy":"Andrew","lastUpdatedAt":1769697564000,"sidebarPosition":7,"frontMatter":{"title":"MCP tools integration","sidebar_position":7,"description":"Integrate Model Context Protocol (MCP) tools with OpenBB agents for enhanced capabilities","keywords":["MCP","Model Context Protocol","tools","function calling","OpenAI","agents"]},"sidebar":"tutorialSidebar","previous":{"title":"Create HTML artifacts","permalink":"/workspace/developers/ai-features/create-html-artifacts"},"next":{"title":"Custom agent features","permalink":"/workspace/developers/ai-features/custom-agent-features"}}');var o=t(74848),a=t(28453),r=t(94331);const i={title:"MCP tools integration",sidebar_position:7,description:"Integrate Model Context Protocol (MCP) tools with OpenBB agents for enhanced capabilities",keywords:["MCP","Model Context Protocol","tools","function calling","OpenAI","agents"]},l=void 0,c={},d=[{value:"Architecture",id:"architecture",level:2},{value:"Query flow",id:"query-flow",level:3},{value:"OpenBB AI SDK",id:"openbb-ai-sdk",level:3},{value:"Core logic",id:"core-logic",level:2}];function p(e){const n={a:"a",code:"code",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",ul:"ul",...(0,a.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(r.A,{title:"AI Features \u2014 MCP Tools Integration | OpenBB Workspace Docs"}),"\n",(0,o.jsx)(n.p,{children:"Enable agents to access and execute Model Context Protocol (MCP) tools, extending their capabilities with external tools and services. MCP tools allow agents to interact with databases, APIs, file systems, and other external resources."}),"\n",(0,o.jsxs)(n.p,{children:["Reference implementation in ",(0,o.jsx)(n.a,{href:"https://github.com/OpenBB-finance/agents-for-openbb/tree/main/38-vanilla-agent-mcp-tools/vanilla_agent_mcp_tools/main.py",children:"this GitHub repository"}),"."]}),"\n",(0,o.jsx)("img",{className:"pro-border-gradient",width:"800",alt:"Charts",src:"https://openbb-cms.directus.app/assets/bec55103-71cd-412d-bc05-fd4776a0838c.png"}),"\n",(0,o.jsx)(n.h2,{id:"architecture",children:"Architecture"}),"\n",(0,o.jsx)(n.p,{children:"MCP tools integration enables agents to discover, execute, and process results from external tools. The agent acts as an orchestrator between the OpenBB workspace and MCP servers, translating tool calls into executable functions."}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.code,{children:"agents.json"})," configuration with MCP support:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'return JSONResponse(content={\n    "vanilla_agent_mcp": {\n        "name": "Vanilla Agent with MCP",\n        "description": "A vanilla agent that supports MCP tools and automatically retrieves widget data.",\n        "endpoints": {"query": "/v1/query"},\n        "features": {\n            "streaming": True,\n            "widget-dashboard-select": True,\n            "widget-dashboard-search": False,\n            "mcp-tools": True,  # Enable MCP tools support\n        },\n    }\n})\n'})}),"\n",(0,o.jsx)(n.h3,{id:"query-flow",children:"Query flow"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"User sends query to agent with available MCP tools in request payload"}),"\n",(0,o.jsx)(n.li,{children:"Agent receives tool metadata including server ID, name, description, and input schema"}),"\n",(0,o.jsx)(n.li,{children:"Agent analyzes query and determines if MCP tools are needed"}),"\n",(0,o.jsx)(n.li,{children:"Agent generates OpenAI function call with appropriate parameters"}),"\n",(0,o.jsx)(n.li,{children:"Frontend executes MCP tool and returns results"}),"\n",(0,o.jsx)(n.li,{children:"Agent processes tool results and generates final response"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"openbb-ai-sdk",children:"OpenBB AI SDK"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"QueryRequest.tools"}),": List of available MCP tools with metadata"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"FunctionCallSSE"}),": Server-sent event for function calls to frontend"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"FunctionCallSSEData"}),": Function call data with server ID, tool name, and parameters"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"message_chunk(text)"}),": Streams response content back to user"]}),"\n",(0,o.jsxs)(n.li,{children:["Tool results received as ",(0,o.jsx)(n.code,{children:'message.role == "tool"'})," in conversation"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"core-logic",children:"Core logic"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'from typing import AsyncGenerator\nimport json\nimport openai\nfrom openbb_ai import message_chunk\nfrom openbb_ai.models import (\n    MessageChunkSSE,\n    QueryRequest,\n    FunctionCallSSE,\n    FunctionCallSSEData,\n)\nfrom sse_starlette.sse import EventSourceResponse\n\n@app.post("/v1/query")\nasync def query(request: QueryRequest) -> EventSourceResponse:\n    """Query the Copilot with MCP tools support."""\n    \n    # Build system prompt with MCP tools information\n    system_content = "You are a helpful financial assistant. Your name is \'Vanilla Agent\'."\n    \n    # Add MCP tools to system prompt if available\n    if request.tools:\n        system_content += "\\n\\nYou have access to the following MCP tools:\\n"\n        for tool in request.tools:\n            server_id = getattr(tool, "server_id", "unknown")\n            system_content += f"\\n- Tool: {tool.name} (Server ID: {server_id})\\n"\n            system_content += f"  Description: {tool.description}\\n"\n            if hasattr(tool, "input_schema") and tool.input_schema:\n                # Parse the schema to identify required parameters\n                if isinstance(tool.input_schema, dict):\n                    properties = tool.input_schema.get("properties", {})\n                    required = tool.input_schema.get("required", [])\n\n                    if properties:\n                        system_content += "  Parameters:\\n"\n                        for param_name, param_info in properties.items():\n                            param_type = param_info.get("type", "unknown")\n                            param_desc = param_info.get("description", "")\n                            is_required = param_name in required\n                            req_str = " (REQUIRED)" if is_required else " (optional)"\n                            system_content += f"    - {param_name}{req_str}: {param_type}"\n                            if param_desc:\n                                system_content += f" - {param_desc}"\n                            system_content += "\\n"\n    \n    # Create OpenAI function definition for MCP tools\n    functions = []\n    if request.tools:\n        functions.append({\n            "name": "execute_agent_tool",\n            "description": "Execute an MCP tool to retrieve data",\n            "parameters": {\n                "type": "object",\n                "properties": {\n                    "server_id": {\n                        "type": "string",\n                        "description": "The ID of the MCP server",\n                        "enum": list(set(\n                            getattr(tool, "server_id", "unknown")\n                            for tool in request.tools\n                        )),\n                    },\n                    "tool_name": {\n                        "type": "string",\n                        "description": "The name of the tool to execute",\n                        "enum": [tool.name for tool in request.tools],\n                    },\n                    "parameters": {\n                        "type": "object",\n                        "description": "The arguments to pass to the tool",\n                        "additionalProperties": True,\n                    },\n                },\n                "required": ["server_id", "tool_name", "parameters"],\n            },\n        })\n    \n    # Handle conversation and tool results\n    openai_messages = [{"role": "system", "content": system_content}]\n    \n    context_str = ""\n    for index, message in enumerate(request.messages):\n        if message.role == "human":\n            openai_messages.append({"role": "user", "content": message.content})\n        elif message.role == "ai":\n            if isinstance(message.content, str):\n                openai_messages.append({"role": "assistant", "content": message.content})\n        # Handle tool results in the conversation context\n        elif message.role == "tool" and index == len(request.messages) - 1:\n            context_str += "## MCP OUTPUT\\n"\n            for result in message.data:\n                for item in result.items:\n                    context_str += f"{item.content}\\n\\n"\n            context_str += "## AI OUTPUT\\n"\n            context_str += "Now provide your analysis based on the MCP output above.\\n\\n"\n    \n    if context_str:\n        openai_messages[-1]["content"] += "\\n\\n" + context_str\n    \n    async def execution_loop() -> AsyncGenerator[MessageChunkSSE | FunctionCallSSE, None]:\n        client = openai.AsyncOpenAI()\n        \n        # Check if we have tool results from previous execution\n        last_message = request.messages[-1] if request.messages else None\n        if last_message and last_message.role == "tool":\n            # Continue conversation with tool results\n            async for event in await client.chat.completions.create(\n                model="gpt-4o",\n                messages=openai_messages,\n                stream=True,\n                # Don\'t pass functions to prevent another tool call\n            ):\n                if chunk := event.choices[0].delta.content:\n                    yield message_chunk(chunk).model_dump()\n            return\n        \n        # Make function call if tools are available\n        if functions:\n            response = await client.chat.completions.create(\n                model="gpt-4o",\n                messages=openai_messages,\n                functions=functions,\n                stream=False,\n            )\n            \n            message = response.choices[0].message\n            \n            # Handle function calls\n            if message.function_call and message.function_call.name == "execute_agent_tool":\n                args = json.loads(message.function_call.arguments)\n                \n                # Send function call back to frontend for MCP execution\n                function_call_data = FunctionCallSSEData(\n                    function="execute_agent_tool",\n                    input_arguments={\n                        "server_id": args.get("server_id", ""),\n                        "tool_name": args.get("tool_name", ""),\n                        "parameters": args.get("parameters", {}),\n                    },\n                    extra_state={\n                        "copilot_function_call_arguments": {\n                            "server_id": args.get("server_id", ""),\n                            "tool_name": args.get("tool_name", ""),\n                            "tool_args": args.get("parameters", {}),\n                            "summary": f"Execute {args.get(\'tool_name\', \'\')} MCP tool",\n                        }\n                    }\n                )\n                \n                yield FunctionCallSSE(data=function_call_data).model_dump()\n                return  # Return control to frontend\n            \n            # If no function call, stream the response\n            if message.content:\n                for char in message.content:\n                    yield message_chunk(char).model_dump()\n        else:\n            # Regular streaming without function calls\n            async for event in await client.chat.completions.create(\n                model="gpt-4o",\n                messages=openai_messages,\n                stream=True,\n            ):\n                if chunk := event.choices[0].delta.content:\n                    yield message_chunk(chunk).model_dump()\n    \n    return EventSourceResponse(\n        content=execution_loop(),\n        media_type="text/event-stream",\n    )\n'})})]})}function u(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(p,{...e})}):p(e)}},94331(e,n,t){t.d(n,{A:()=>a});t(96540);var s=t(5260),o=t(74848);function a({title:e}){return(0,o.jsx)(s.A,{children:(0,o.jsx)("title",{children:e})})}}}]);